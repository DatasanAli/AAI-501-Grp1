{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid warnings caused by inplace column name reassignment\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# fetch dataset \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "features = default_of_credit_card_clients.data.features\n",
    "targets = default_of_credit_card_clients.data.targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename feature columns to be more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for better understanding\n",
    "features.rename(inplace=True,\n",
    "                columns={\n",
    "                    'X1': 'CREDIT_LIMIT',  # Credit limit (NT dollar)\n",
    "                    'X2': 'GENDER',  # Gender (1 = male; 2 = female)\n",
    "                    'X3': 'EDUCATION_LEVEL',  # Education (1 = graduate school; 2 = university; 3 = high school; 4 = others)\n",
    "                    'X4': 'MARITAL_STATUS',  # Marital status (1 = married; 2 = single; 3 = others)\n",
    "                    'X5': 'AGE',  # (years)\n",
    "\n",
    "                    # X6 - X11 is repayment status\n",
    "                    # The measurement scale for the repayment status is:\n",
    "                    # -1 = pay duly;\n",
    "                    # 1 = payment delay for one month;\n",
    "                    # 2 = payment delay for two months;\n",
    "                    # . . .;\n",
    "                    # 8 = payment delay for eight months;\n",
    "                    # 9 = payment delay for nine months and above.\n",
    "                    'X6': 'SEPT_PAY_STATUS',  # repayment status in September, 2005\n",
    "                    'X7': 'AUG_PAY_STATUS',  # repayment status in August, 2005\n",
    "                    'X8': 'JULY_PAY_STATUS',  # repayment status in July, 2005\n",
    "                    'X9': 'JUNE_PAY_STATUS',  # repayment status in June, 2005\n",
    "                    'X10': 'MAY_PAY_STATUS',  # repayment status in May, 2005\n",
    "                    'X11': 'APRIL_PAY_STATUS',  # repayment status in April, 2005\n",
    "\n",
    "                    # X12 - X17 is amount of bill statement (NT dollar)\n",
    "                    'X12': 'SEPT_BILL',  # amount of bill statement in September, 2005\n",
    "                    'X13': 'AUG_BILL',  # amount of bill statement in August, 2005\n",
    "                    'X14': 'JULY_BILL',  # amount of bill statement in July, 2005\n",
    "                    'X15': 'JUNE_BILL',  # amount of bill statement in June, 2005\n",
    "                    'X16': 'MAY_BILL',  # amount of bill statement in May, 2005\n",
    "                    'X17': 'APRIL_BILL',  # amount of bill statement in April, 2005\n",
    "\n",
    "                    # X18 - X23 is amount of previous payment (NT dollar)\n",
    "                    'X18': 'SEPT_PAYMENT',  # amount paid in September, 2005\n",
    "                    'X19': 'AUG_PAYMENT',  # amount paid in August, 2005\n",
    "                    'X20': 'JULY_PAYMENT',  # amount paid in July, 2005\n",
    "                    'X21': 'JUNE_PAYMENT',  # amount paid in June, 2005\n",
    "                    'X22': 'MAY_PAYMENT',  # amount paid in May, 2005\n",
    "                    'X23': 'APRIL_PAYMENT',  # amount paid in April, 2005\n",
    "                })\n",
    "\n",
    "# Display the updated dataframe\n",
    "features.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename target column to be more descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.rename(inplace = True,\n",
    "               columns={'Y': 'DEFAULT'} # Default payment next month\n",
    "              )\n",
    "\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and targets for EDA\n",
    "data = pd.concat([features, targets], axis=1)\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Plot distributions of numerical variables\n",
    "numerical_features = [\n",
    "    'CREDIT_LIMIT', 'AGE', 'SEPT_BILL', 'AUG_BILL', 'JULY_BILL',\n",
    "    'JUNE_BILL', 'MAY_BILL', 'APRIL_BILL', 'SEPT_PAYMENT',\n",
    "    'AUG_PAYMENT', 'JULY_PAYMENT', 'JUNE_PAYMENT', 'MAY_PAYMENT',\n",
    "    'APRIL_PAYMENT'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "data[numerical_features].hist(bins=20, figsize=(20, 15))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(data, columns):\n",
    "    for column in columns:\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)][column]\n",
    "        print(f\"{column}: {len(outliers)} outliers capped.\")\n",
    "        data[column] = data[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return data\n",
    "\n",
    "features = cap_outliers(features, numerical_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "features['GENDER'] = features['GENDER'].map({1: 0, 2: 1})  # 1: Male -> 0, 2: Female -> 1\n",
    "print(\"GENDER column transformed:\", features['GENDER'].unique())\n",
    "\n",
    "features['EDUCATION_LEVEL'] = features['EDUCATION_LEVEL'].replace({0: 4, 5: 4, 6: 4})  # 4: Others\n",
    "print(\"EDUCATION_LEVEL column transformed:\", features['EDUCATION_LEVEL'].unique())\n",
    "\n",
    "features['MARITAL_STATUS'] = features['MARITAL_STATUS'].replace({0: 3})  # 3: Others\n",
    "print(\"MARITAL_STATUS column transformed:\", features['MARITAL_STATUS'].unique())\n",
    "\n",
    "# Combine processed features with targets\n",
    "processed_data = pd.concat([features, targets], axis=1)\n",
    "print(\"Processed data sample:\")\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Data After Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical columns using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features[numerical_features] = scaler.fit_transform(features[numerical_features])\n",
    "\n",
    "# Combine normalized features with targets for inspection\n",
    "normalized_data = pd.concat([features, targets], axis=1)\n",
    "\n",
    "# Display normalized data sample\n",
    "print(\"Normalized data sample:\")\n",
    "print(normalized_data.head())\n",
    "\n",
    "# Check the initial class distribution\n",
    "print(\"\\nOriginal class distribution:\")\n",
    "print(targets.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a test set\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(\n",
    "    features, targets['DEFAULT'], test_size=0.2, random_state=42, stratify=targets['DEFAULT']\n",
    ")\n",
    "\n",
    "# Step 2: Perform K-Fold Cross-Validation on the remaining train+validation set\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "train_indices = []\n",
    "validation_indices = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train_val)):\n",
    "    train_indices.append(train_idx)\n",
    "    validation_indices.append(val_idx)\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"  Training indices: {train_idx[:5]}... ({len(train_idx)} samples)\")\n",
    "    print(f\"  Validation indices: {val_idx[:5]}... ({len(val_idx)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply K-Means SMOTE and Train the Model in Each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create train/validation and test sets\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(\n",
    "    features, targets['DEFAULT'], test_size=0.2, random_state=42, stratify=targets['DEFAULT']\n",
    ")\n",
    "\n",
    "# Step 2: Initialize KMeansSMOTE and GaussianNB\n",
    "kmeans_smote = KMeansSMOTE(random_state=0, k_neighbors=5, cluster_balance_threshold=0.1)\n",
    "model = GaussianNB()  # Replace with your preferred model\n",
    "\n",
    "# Step 3: Perform K-Fold Cross-Validation for train/validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "# Store scores for each fold\n",
    "train_scores = []\n",
    "validation_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train_val)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Get training and validation data for the current fold\n",
    "    x_fold_train, x_fold_val = x_train_val.iloc[train_idx], x_train_val.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train_val.iloc[train_idx], y_train_val.iloc[val_idx]\n",
    "    \n",
    "    # Apply KMeansSMOTE to the training data\n",
    "    x_fold_train_resampled, y_fold_train_resampled = kmeans_smote.fit_resample(x_fold_train, y_fold_train)\n",
    "    \n",
    "    # Train the model on the resampled training data\n",
    "    model.fit(x_fold_train_resampled, y_fold_train_resampled)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    y_train_pred = model.predict(x_fold_train_resampled)\n",
    "    train_accuracy = accuracy_score(y_fold_train_resampled, y_train_pred)\n",
    "    train_scores.append(train_accuracy)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = model.predict(x_fold_val)\n",
    "    val_accuracy = accuracy_score(y_fold_val, y_val_pred)\n",
    "    validation_scores.append(val_accuracy)\n",
    "    \n",
    "    print(f\"  Training Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Step 4: Calculate average scores across all folds\n",
    "average_train_score = sum(train_scores) / n_splits\n",
    "average_val_score = sum(validation_scores) / n_splits\n",
    "\n",
    "print(\"\\nK-Fold Cross-Validation with K-Means SMOTE Results:\")\n",
    "print(f\"  Average Training Accuracy: {average_train_score:.2f}\")\n",
    "print(f\"  Average Validation Accuracy: {average_val_score:.2f}\")\n",
    "\n",
    "# Step 5: Final evaluation on the test set\n",
    "x_test_resampled, y_test_resampled = kmeans_smote.fit_resample(x_train_val, y_train_val)  # Fit SMOTE to the full train/val data\n",
    "model.fit(x_test_resampled, y_test_resampled)  # Retrain on full train/val data\n",
    "\n",
    "# Test set evaluation\n",
    "y_test_pred = model.predict(x_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Set Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features (check if this is needed)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform Recursive Feature Elimination (RFE)\n",
    "rfe_logistic = RFE(estimator=logistic_model, n_features_to_select=1, step=1)\n",
    "fit_logistic = rfe_logistic.fit(features_scaled, targets['DEFAULT'].values.ravel())\n",
    "\n",
    "# Create a DataFrame for feature rankings\n",
    "all_features_ranking = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Ranking': fit_logistic.ranking_,\n",
    "}).sort_values(by='Ranking')\n",
    "\n",
    "# Display top-ranked features\n",
    "print(\"Feature Ranking (Top 10):\")\n",
    "print(all_features_ranking.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline XGBoost with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "xgb_model_all_features = XGBClassifier(eval_metric='logloss')\n",
    "xgb_model_all_features.fit(features_train, target_train)\n",
    "\n",
    "pred = xgb_model_all_features.predict(features_test)\n",
    "score = accuracy_score(target_test, pred)\n",
    "print('Accuracy score using all features: {}'.format(f\"{score:.4f}\"))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(target_test, pred))\n",
    "\n",
    "plot_importance(xgb_model_all_features, importance_type='gain')\n",
    "plt.title('Feature Importance by Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with top 10 features by Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = xgb_model_all_features.get_booster().get_score(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame.from_dict(feature_importance, orient='index', columns=['Gain']).sort_values(by='Gain', ascending=False)\n",
    "top_10_features = feature_importance_df.head(10).index.tolist()\n",
    "\n",
    "xgb_model_top_10_features = XGBClassifier(eval_metric='logloss')\n",
    "xgb_model_top_10_features.fit(features_train[top_10_features], target_train)\n",
    "\n",
    "pred = xgb_model_top_10_features.predict(features_test[top_10_features])\n",
    "score = accuracy_score(target_test, pred)\n",
    "print('Accuracy score using top 10 features: {}'.format(score))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(target_test, pred))\n",
    "\n",
    "plot_importance(xgb_model_top_10_features, importance_type='gain')\n",
    "plt.title('Feature Importance by Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost without demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_no_demographics = features_train.drop(['GENDER', 'AGE', 'EDUCATION_LEVEL', 'MARITAL_STATUS'], axis=1)\n",
    "features_test_no_demographics = features_test.drop(['GENDER', 'AGE', 'EDUCATION_LEVEL', 'MARITAL_STATUS'], axis=1)\n",
    "\n",
    "xgb_model_no_demographics = XGBClassifier(eval_metric='logloss')\n",
    "xgb_model_no_demographics.fit(features_train_no_demographics, target_train)\n",
    "\n",
    "pred = xgb_model_no_demographics.predict(features_test_no_demographics)\n",
    "score = accuracy_score(target_test, pred)\n",
    "print('Accuracy score using all features: {}'.format(score))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(target_test, pred))\n",
    "\n",
    "plot_importance(xgb_model_no_demographics, importance_type='gain')\n",
    "plt.title('Feature Importance by Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use only 3 most recent months in features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop recent month-related bill and payment features\n",
    "recent_features = ['APRIL_BILL', 'APRIL_PAYMENT', 'MAY_BILL', 'MAY_PAYMENT', 'JUNE_BILL', 'JUNE_PAYMENT']\n",
    "features_train_no_demo_most_recent = features_train_no_demographics.drop(recent_features, axis=1)\n",
    "features_test_no_demo_most_recent = features_test_no_demographics.drop(recent_features, axis=1)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model_no_demo_most_recent = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "xgb_model_no_demo_most_recent.fit(features_train_no_demo_most_recent, target_train)\n",
    "\n",
    "# Evaluate the model\n",
    "pred = xgb_model_no_demo_most_recent.predict(features_test_no_demo_most_recent)\n",
    "score = accuracy_score(target_test, pred)\n",
    "print(f'Accuracy score without demographics and recent months: {score:.4f}')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(target_test, pred))\n",
    "\n",
    "# Plot feature importance\n",
    "plot_importance(xgb_model_no_demo_most_recent, importance_type='gain', max_num_features=10, height=0.5)\n",
    "plt.title('Top 10 Feature Importance by Gain (Excluding Demographics and Recent Months)', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance via SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost with all features \n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(xgb_model_all_features, x_fold_train_resampled)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(x_fold_train_resampled)\n",
    "\n",
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, x_fold_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost with no demographics \n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(xgb_model_no_demographics, x_fold_train_resampled)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(x_fold_train_resampled)\n",
    "\n",
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, x_fold_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Contribution for a Single Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled_df = pd.DataFrame(x_fold_train_resampled, columns=features.columns)\n",
    "\n",
    "# Choose a specific instance from the test set\n",
    "## Why couldnt we pick aggregate (all rows) instead of indexing at 0 and only picking 1 row? \n",
    "instance_index = 0  # Adjust the index as needed\n",
    "instance_data = x_test_scaled_df.iloc[instance_index:instance_index+1] #+1 is to extract row as dataframe instead of a series \n",
    "\n",
    "# Generate SHAP values for the instance\n",
    "shap_values_instance = explainer.shap_values(instance_data)\n",
    "\n",
    "# SHAP waterfall plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=shap_values_instance[0],\n",
    "        base_values=explainer.expected_value,\n",
    "        data=instance_data.iloc[0],\n",
    "        feature_names=features.columns  # Ensures feature names are shown\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the normalized data\n",
    "\n",
    "# 1. Boxplots of normalized numerical features\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=features[numerical_features])\n",
    "plt.title(\"Boxplots of Normalized Numerical Features\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 2. Histograms of normalized features\n",
    "features[numerical_features].hist(bins=20, figsize=(15, 10))\n",
    "plt.suptitle(\"Histograms of Normalized Numerical Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize the correlation matrix\n",
    "normalized_correlation_matrix = features[numerical_features].corr()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(normalized_correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix After Normalization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a simple Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(features[numerical_features], targets['DEFAULT'])\n",
    "\n",
    "# Plot feature importance\n",
    "importances = pd.Series(rf_model.feature_importances_, index=numerical_features)\n",
    "importances.sort_values().plot(kind='barh', figsize=(10, 6), title=\"Feature Importance (Random Forest)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_target = processed_data.corr()['DEFAULT'].sort_values(ascending=False)\n",
    "print(\"Correlation of Features with Target:\")\n",
    "print(correlation_with_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats_msaai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
